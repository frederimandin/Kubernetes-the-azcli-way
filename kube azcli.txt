## still needs a lot of comments :)

# Set the private variables
AZURE_SUBSCRIPTION=Id_of_the_correct_Azure_subscription
SSH_PASSWORD=Password_for_SSH_connection_to_the_hosts
SSH_USERNAME=Username_to_use_for_SSH_connection
GROUP=Name_of_the_resource_group
VNET_NAME=Name_of_the_Vnet
NSG_NAME=Name_of_the_NSG
SUBNET_NAME=Name_fo_the_Subnet
PUBLIC_IP_NAME=Name_of_the_Public_IP
LOAD_BALANCER=Name_of_the_lb
POOL_NAME=Name_of_the_LB_pool
RULE_NAME=Name_fo_the_load_balancer_rule
PROBE_NAME=Name_of_the_probe
ASET_NAME=Name_of_the_availability_set

# Set the correct Azure subscription id
az account set -s $AZURE_SUBSCRIPTION

# create the ressource groupe in Wet Europe
az group create -l westeurope -n $GROUP

#create the availability-set that is need by the load balancer for the API frontend
az vm availability-set create -n $ASET_NAME -g $GROUP -l westeurope

#create the public IP for the cluster, and store this IP into the dedicated variable
az network public-ip create -n $PUBLIC_IP_NAME -l westeurope -g $GROUP  --allocation-method static
KUBERNETES_PUBLIC_ADDRESS=$(az network public-ip show -g $GROUP -n $PUBLIC_IP_NAME --query "{ address: ipAddress }" -o tsv)

# create the controllers public IPS
az network public-ip create -n controller0IP -l westeurope -g $GROUP  --allocation-method static
az network public-ip create -n controller1IP -l westeurope -g $GROUP  --allocation-method static
az network public-ip create -n controller2IP -l westeurope -g $GROUP  --allocation-method static

#Create the network infrastructure : vnet, subnet and NSG
az network vnet create -n $VNET_NAME -g $GROUP
az network vnet update -g $GROUP -n $VNET_NAME --address-prefixes 10.240.0.0/16 10.200.0.0/16
az network nsg create -n $NSG_NAME -g $GROUP -l westeurope
az network vnet subnet create -g $GROUP --vnet-name $VNET_NAME -n $SUBNET_NAME --address-prefix 10.240.0.0/24 --network-security-group $NSG_NAME
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-internal --priority 100 --access Allow --source-address-prefix 10.240.0.0/24
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-internal2 --priority 101 --access Allow --source-address-prefix 10.200.0.0/16
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-external3 --priority 102 --access Allow --destination-port-range 22
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-external --priority 103 --access Allow --destination-port-range 3389
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-external2 --priority 104 --access Allow --destination-port-range 6443
az network nsg rule create -g $GROUP --nsg-name $NSG_NAME -n allow-healthz --priority 105 --access Allow --destination-port-range 8080

# Create the API frontend load balancer, frontend IP, probe and load balancing rule
az network lb create -g $GROUP -n $LOAD_BALANCER -l westeurope --backend-pool-name $POOL_NAME --public-ip-address $PUBLIC_IP_NAME  --public-ip-address-type Public --vnet-name $VNET_NAME --subnet $SUBNET_NAME
az network lb probe create --lb-name $LOAD_BALANCER -n $PROBE_NAME --port 8080 --protocol http -g $GROUP --path /healthz
az network lb rule create --backend-port 6443 --frontend-port 6443 --lb-name $LOAD_BALANCER -n $RULE_NAME --protocol tcp -g $GROUP --backend-pool-name $POOL_NAME --probe-name $PROBE_NAME
 
# create the NICs attached to the load balancer pools
az network nic create -g $GROUP -n controller0VMNic --vnet-name $VNET_NAME --subnet $SUBNET_NAME --lb-name $LOAD_BALANCER --lb-address-pools $POOL_NAME --private-ip-address 10.240.0.10 --public-ip-address controller0IP
az network nic create -g $GROUP -n controller1VMNic --vnet-name $VNET_NAME --subnet $SUBNET_NAME --lb-name $LOAD_BALANCER --lb-address-pools $POOL_NAME --private-ip-address 10.240.0.11 --public-ip-address controller1IP
az network nic create -g $GROUP -n controller2VMNic --vnet-name $VNET_NAME --subnet $SUBNET_NAME --lb-name $LOAD_BALANCER --lb-address-pools $POOL_NAME --private-ip-address 10.240.0.12 --public-ip-address controller2IP

#Create the Controller VMs
az vm create -n controller0 -g $GROUP --image UbuntuLTS --availability-set $ASET_NAME --nics controller0VMNic --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --nsg ""
az vm create -n controller1 -g $GROUP --image UbuntuLTS --availability-set $ASET_NAME --nics controller1VMNic --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --nsg ""
az vm create -n controller2 -g $GROUP --image UbuntuLTS --availability-set $ASET_NAME --nics controller2VMNic --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --nsg ""


# fill the cloud-config response file. File is used to auto install socat in the worker VMs
echo "#cloud-config" >> cloud-config.txt
echo "runcmd:" >> cloud-config.txt
echo "	- [apt-get, update]" >> cloud-config.txt
echo "	- [apt-get, install, -y, socat]" >> cloud-config.txt

# create the three worker VMs
az vm create -n worker0 -g $GROUP --image UbuntuLTS --vnet-name $VNET_NAME --subnet $SUBNET_NAME --private-ip-address 10.240.0.20 --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --custom-data cloud-config.txt --nsg ""
az vm create -n worker1 -g $GROUP --image UbuntuLTS --vnet-name $VNET_NAME --subnet $SUBNET_NAME --private-ip-address 10.240.0.21 --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --custom-data cloud-config.txt --nsg ""
az vm create -n worker2 -g $GROUP --image UbuntuLTS --vnet-name $VNET_NAME --subnet $SUBNET_NAME --private-ip-address 10.240.0.22 --authentication-type password --admin-username $SSH_USERNAME --admin-password $SSH_PASSWORD --custom-data cloud-config.txt --nsg ""




## to be executed after certificate creation : https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/02-certificate-authority.md

# copy the certificates to the worker nodes
for host in worker0 worker1 worker2; do
	HOSTIP=$(az vm list-ip-addresses -g $GROUP -n ${host} --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress' -o tsv)
  scp ca.pem kube-proxy.pem kube-proxy-key.pem ${HOSTIP}:~/
done

# copy the certificates to the controller nodes
for host in controller0 controller1 controller2; do
	HOSTIP=$(az vm list-ip-addresses -g $GROUP -n ${host} --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress' -o tsv)
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem ${HOSTIP}:~/
done

## to be executed after tokens and bootstrap files creation : https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/03-auth-configs.md
# copy the token to the controller nodes
for host in controller0 controller1 controller2; do
	HOSTIP=$(az vm list-ip-addresses -g $GROUP -n ${host} --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress' -o tsv)
  scp token.csv ${HOSTIP}:~/
done

#copy the kubeconfig files to the worker nodes
for host in worker0 worker1 worker2; do
	HOSTIP=$(az vm list-ip-addresses -g $GROUP -n ${host} --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress' -o tsv)
  scp bootstrap.kubeconfig kube-proxy.kubeconfig ${HOSTIP}:~/
done

##needed during build phase of etcd : https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-etcd.md
INTERNAL_IP=$(hostname -I | cut -d' ' -f1)
ETCD_NAME=controller$(echo $INTERNAL_IP | cut -c 11)


